{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "# this requires data to be in libsvm format, good for sparse vectors but our vector has 4 points\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/tree.html\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# do not truncate arrays\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# see creation of decision tree training data: https://github.com/nuria/study/tree/master/poc-bot-classifier\n",
    "# df = spark.sql(\" drop table if exists nuria.classifier_data_features;\") \\\n",
    "# df = spark.sql(\"create table nuria.classifier_data_features as select session_length_secs, number_of_requests, \\\n",
    "# nocookies, label  from nuria.classifier_data_processed\")\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "#iris = datasets.load_iris()\n",
    "\n",
    "#print (iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "| count(1)|    label|\n",
      "+---------+---------+\n",
      "|   178373|automated|\n",
      "|   823669|     null|\n",
      "|164336104|     user|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see how balanced are tables in terms of data\n",
    "\n",
    "df = spark.sql(\"select count(*), label from nuria.classifier_data_sorted_processed group by label\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|count(1)|    label|\n",
      "+--------+---------+\n",
      "|     398|automated|\n",
      "|    1124|     null|\n",
      "|   98478|     user|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select count(*), label from nuria.classifier_training_data_human_sorted_processed group by label\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|count(1)|    label|\n",
      "+--------+---------+\n",
      "|    2085|automated|\n",
      "|     257|     null|\n",
      "|    5488|     user|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select count(*), label from nuria.classifier_training_data_bot_sorted_processed group by label\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+------------------+---------------------+---------+-----------------+---------+\n",
      "|sessionid                       |session_length_secs|number_of_requests|request_ratio_per_min|nocookies|user_agent_length|label    |\n",
      "+--------------------------------+-------------------+------------------+---------------------+---------+-----------------+---------+\n",
      "|0000bdb4077f6a7c1172244e623f882e|0                  |1                 |null                 |0.0      |70               |user     |\n",
      "|00016ebd5652fcbcd169f742b98fb9ec|80627              |26                |0                    |26.0     |1                |automated|\n",
      "|0001bbaa864fe48798b49af1eee86af3|0                  |1                 |null                 |0.0      |68               |user     |\n",
      "|0001eb4aad9f994f6c3ccffe9303249a|27862              |17                |0                    |17.0     |11               |automated|\n",
      "|0003093bbf6b976417c999e57aa38f3f|417                |7                 |1                    |0.0      |70               |user     |\n",
      "|00032774a8ce69bffbc73a91350193ce|31982              |17                |0                    |17.0     |11               |automated|\n",
      "|000348c7458a4554c16430199f6f7bbd|42836              |6                 |0                    |0.0      |66               |user     |\n",
      "|0003639ff2c770497fc455ee21363531|0                  |1                 |null                 |0.0      |70               |user     |\n",
      "|000520fe6304048bbe15b96ed72d0d95|0                  |1                 |null                 |0.0      |66               |user     |\n",
      "|0005386b7a7ad43132c5261285591964|0                  |1                 |null                 |0.0      |68               |user     |\n",
      "+--------------------------------+-------------------+------------------+---------------------+---------+-----------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from nuria.model_training_data order by sessionId\")\n",
    "df.show(10, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hive (nuria)> desc model_training_data;\n",
    "OK\n",
    "col_name\tdata_type\tcomment\n",
    "session_length_secs \tbigint\n",
    "number_of_requests  \tbigint\n",
    "request_ratio_per_min\tint\n",
    "nocookies           \tdouble\n",
    "user_agent_length   \tint\n",
    "label               \tstring\n",
    "'''\n",
    "\n",
    "\n",
    "# move data into  two arrays\n",
    "# As with other classifiers, DecisionTreeClassifier takes as input \n",
    "# two arrays: an array X, sparse or dense, of size [n_samples, n_features] holding the training samples, and an array \n",
    "# Y of integer values, size [n_samples], holding the class labels for the training samples:\n",
    "\n",
    "df = spark.sql(\"select * from nuria.model_training_data order by sessionId\")\n",
    "\n",
    "def format_input_data(df):\n",
    "    # vector of labels, they need to be integer values\n",
    "    # we use 1 for automated, 0 for user\n",
    "    labels = []\n",
    "    # array with feature value, a matrix of [samples (rows) * features(feature cardinality)]\n",
    "    features = []\n",
    "\n",
    "    for row in df.collect():\n",
    "        datapoint_label = row[\"label\"]\n",
    "        if datapoint_label==\"automated\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            \n",
    "        if row[\"request_ratio_per_min\"] == None:\n",
    "            request_ratio_per_min = 0\n",
    "        else:\n",
    "            request_ratio_per_min = row[\"request_ratio_per_min\"]\n",
    "            \n",
    "        datapoint_features = [row[\"session_length_secs\"], row[\"number_of_requests\"],request_ratio_per_min, row[\"nocookies\"], row[\"user_agent_length\"]]\n",
    "        \n",
    "        features.append(datapoint_features)\n",
    "        \n",
    "    return (labels, features)\n",
    "\n",
    "#print (labels)\n",
    "\n",
    "(labels, features) = format_input_data(df)\n",
    "\n",
    "#print (features) \n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.plot_tree(clf.fit(features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218 of sessions are labeled as automated, 1782 are labelled as user \n"
     ]
    }
   ],
   "source": [
    "# get sessions from our testing data and see how are those classified\n",
    "\n",
    "df_testing_data = spark.sql(\"select * from nuria.model_testing_data\")\n",
    "\n",
    "(labels_testing_data, features_testing_data) = format_input_data(df_testing_data)\n",
    "\n",
    "predicted_labels = clf.predict(features_testing_data)\n",
    "\n",
    "automated = 0\n",
    "user = 0\n",
    "\n",
    "for item in predicted_labels:\n",
    "    if item == 0:\n",
    "        automated = automated + 1\n",
    "    elif item == 1:\n",
    "        user = user + 1 \n",
    "\n",
    "print ( \"{0} of sessions are labeled as automated, {1} are labelled as user \".format(automated, user ))\n",
    "                            \n",
    "# print( predicted_labels )\n",
    "\n",
    "# print(labels_testing_data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
